{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "nonprofit-delaware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from neural_tangents import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baking-duration",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(861, dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of Jax warnings up front\n",
    "jnp.arange(42).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-stocks",
   "metadata": {},
   "source": [
    "# Reference implementation\n",
    "Using `neural-tangents`\n",
    "\n",
    "```python\n",
    "stax.serial(\n",
    "    stax.Dense(1, W_std=W_std, b_std=b_std), stax.Relu(),\n",
    "    stax.Dense(1, W_std=W_std, b_std=b_std), stax.Relu(),\n",
    "    stax.Dense(1, W_std=W_std, b_std=0)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "toxic-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_kernel_fn(\n",
    "    num_hidden_layers: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    layers = []\n",
    "    for _ in range(num_hidden_layers):\n",
    "        layers.append(stax.Dense(1, W_std=W_std, b_std=b_std))\n",
    "        layers.append(stax.Relu())\n",
    "    layers.append(stax.Dense(1, W_std=W_std, b_std=0))\n",
    "    \n",
    "    _, _, kernel_fn = stax.serial(*layers)\n",
    "    return kernel_fn\n",
    "\n",
    "\n",
    "def mlp_nngp_ref(\n",
    "    xs: jnp.ndarray,\n",
    "    num_hidden_layers: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    kernel_fn = _get_kernel_fn(\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        W_std=W_std,\n",
    "        b_std=b_std,\n",
    "    )\n",
    "    \n",
    "    return kernel_fn(xs, xs).nngp\n",
    "\n",
    "\n",
    "def mlp_ntk_ref(\n",
    "    xs: jnp.ndarray,\n",
    "    num_hidden_layers: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    kernel_fn = _get_kernel_fn(\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        W_std=W_std,\n",
    "        b_std=b_std,\n",
    "    )\n",
    "    \n",
    "    return kernel_fn(xs, xs).ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "difficult-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xs:\n",
      " [[ 0.51232451 -0.76248157  0.47484656 -1.44580725 -0.01293077]\n",
      " [-0.01876019  0.96316201 -1.30182157  0.92914455  0.27818229]\n",
      " [-0.41780672  0.07287297  1.92944858 -1.29686923 -0.40593064]]\n",
      "NNGP:\n",
      " [[0.95399649 0.87331003 0.95016414]\n",
      " [0.87331003 0.96408668 0.88058314]\n",
      " [0.95016414 0.88058314 1.01873236]]\n",
      "NTK:\n",
      " [[2.56598596 1.86727954 2.28452466]\n",
      " [1.86727954 2.60634674 1.83368718]\n",
      " [2.28452466 1.83368718 2.82492942]]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(1))\n",
    "xs = jax.random.normal(key=key, shape=(3, 5))\n",
    "\n",
    "print(\"xs:\\n\", xs)\n",
    "print(\"NNGP:\\n\", mlp_nngp_ref(xs, num_hidden_layers=3, W_std=1, b_std=1))\n",
    "print(\"NTK:\\n\", mlp_ntk_ref(xs, num_hidden_layers=3, W_std=1, b_std=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-executive",
   "metadata": {},
   "source": [
    "# Our implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-exchange",
   "metadata": {},
   "source": [
    "### NNGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "military-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_relu(K: jnp.ndarray):\n",
    "    \"\"\"K should be square\"\"\"\n",
    "    sqrt = jnp.sqrt(\n",
    "        jnp.diag(K)[:, jnp.newaxis]\n",
    "      * jnp.diag(K)[jnp.newaxis, :]\n",
    "    )\n",
    "    \n",
    "    c = K / sqrt\n",
    "    \n",
    "    return 1 / (2 * jnp.pi) * (\n",
    "        jnp.sqrt(1 - c * c)\n",
    "        + (jnp.pi - jnp.arccos(c)) * c\n",
    "    ) * sqrt\n",
    "\n",
    "def mlp_nngp_ours(\n",
    "    xs: jnp.ndarray,\n",
    "    num_hidden_layers: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    assert num_hidden_layers > 0\n",
    "    \n",
    "    data_dim = xs.shape[1]\n",
    "    W_var = W_std ** 2\n",
    "    b_var = b_std ** 2\n",
    "    \n",
    "    # Initial covariance in data space\n",
    "    K_init = xs @ xs.T\n",
    "    \n",
    "    # Kernel after first hidden layer (before ReLU)\n",
    "    K = W_var * K_init / data_dim + b_var\n",
    "    \n",
    "    # Remaining hidden layers\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        K = W_var * v_relu(K) + b_var\n",
    "    \n",
    "    # Final linear output layer\n",
    "    K = W_var * v_relu(K)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unable-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNGP (ref):\n",
      " [[1.61222484 1.4432761  1.60487298]\n",
      " [1.4432761  1.63385407 1.46039207]\n",
      " [1.60487298 1.46039207 1.75099192]]\n",
      "NNGP (ours):\n",
      " [[1.61222484 1.4432761  1.60487298]\n",
      " [1.4432761  1.63385407 1.46039207]\n",
      " [1.60487298 1.46039207 1.75099192]]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(1))\n",
    "xs = jax.random.normal(key=key, shape=(3, 5))\n",
    "\n",
    "print(\"NNGP (ref):\\n\", mlp_nngp_ref(xs, num_hidden_layers=3, W_std=1.1, b_std=1.1))\n",
    "print(\"NNGP (ours):\\n\", mlp_nngp_ours(xs, num_hidden_layers=3, W_std=1.1, b_std=1.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "finnish-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stronger tests\n",
    "for num_hidden_layers in [1, 4, 20]:\n",
    "    key, _ = jax.random.split(jax.random.PRNGKey(num_hidden_layers))\n",
    "    xs = jax.random.normal(key=key, shape=(10, 5))\n",
    "    \n",
    "    assert jnp.allclose(\n",
    "        mlp_nngp_ref(xs, num_hidden_layers=num_hidden_layers,\n",
    "                     W_std=jnp.pi, b_std=jnp.e),\n",
    "        mlp_nngp_ours(xs, num_hidden_layers=num_hidden_layers,\n",
    "                      W_std=jnp.pi, b_std=jnp.e),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-cancellation",
   "metadata": {},
   "source": [
    "### NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "right-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_relu_prime(K: jnp.ndarray):\n",
    "    \"\"\"K should be square\"\"\"\n",
    "    sqrt = jnp.sqrt(\n",
    "        jnp.diag(K)[:, jnp.newaxis]\n",
    "      * jnp.diag(K)[jnp.newaxis, :]\n",
    "    )\n",
    "    \n",
    "    c = K / sqrt\n",
    "    \n",
    "    return 1 / (2 * jnp.pi) * (jnp.pi - jnp.arccos(c))\n",
    "\n",
    "def mlp_ntk_ours(\n",
    "    xs: jnp.ndarray,\n",
    "    num_hidden_layers: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    assert num_hidden_layers > 0\n",
    "    \n",
    "    batch_size = xs.shape[0]\n",
    "    data_dim = xs.shape[1]\n",
    "    W_var = W_std ** 2\n",
    "    b_var = b_std ** 2\n",
    "    \n",
    "    # Initial covariance in data space\n",
    "    K_init = xs @ xs.T\n",
    "    \n",
    "    ############################## Begin forward pass\n",
    "    \n",
    "    # forward_Ks[i] is the coordinate covariance\n",
    "    # of the output after the ith (zero-indexed) weight matrix\n",
    "    # (before the next ReLU activation)\n",
    "    forward_Ks = [W_var * K_init / data_dim + b_var]\n",
    "    for i in range(num_hidden_layers - 1):\n",
    "        forward_Ks.append(\n",
    "            W_var * v_relu(forward_Ks[-1]) + b_var\n",
    "        )\n",
    "    \n",
    "    # out_K is the covariance of the output\n",
    "    # it is the NNGP kernel\n",
    "    out_K = W_var * v_relu(forward_Ks[-1])\n",
    "            \n",
    "    ############################## End forward pass\n",
    "    \n",
    "    ############################## Begin backward pass\n",
    "    \n",
    "    ntk = jnp.zeros((batch_size, batch_size))\n",
    "    \n",
    "    # output layer weights\n",
    "    ntk += out_K\n",
    "    \n",
    "    # Work backwards through hidden layers (0-indexed)\n",
    "    # cur_grad_K[i][j] gives the limit of the inner product\n",
    "    # of the gradients of the current layer's pre-activation\n",
    "    # for inputs xs[i] and xs[j]\n",
    "    cur_grad_K = jnp.ones_like(ntk)\n",
    "    for i in range(num_hidden_layers - 1, -1, -1):\n",
    "        cur_grad_K *= W_var * v_relu_prime(forward_Ks[i])\n",
    "        \n",
    "        # weights\n",
    "        ntk += cur_grad_K * ((forward_Ks[i] - b_var) / W_var)\n",
    "        # ntk += cur_grad_K * forward_Ks[i]\n",
    "        \n",
    "        # bias\n",
    "        # Commented out because neural-tangents assumes not trainable biases\n",
    "        #ntk += cur_grad_K\n",
    "    \n",
    "    ############################## End backward pass\n",
    "    \n",
    "    return ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "difficult-server",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTK (ref):\n",
      " [[0.31598596 0.05541803 0.20624964]\n",
      " [0.05541803 0.35634674 0.07449831]\n",
      " [0.20624964 0.07449831 0.57492942]]\n",
      "NTK (ours):\n",
      " [[0.31598596 0.05541803 0.20624964]\n",
      " [0.05541803 0.35634674 0.07449831]\n",
      " [0.20624964 0.07449831 0.57492942]]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(1))\n",
    "xs = jax.random.normal(key=key, shape=(3, 5))\n",
    "\n",
    "print(\"NTK (ref):\\n\", mlp_ntk_ref(xs, num_hidden_layers=3, W_std=1, b_std=0))\n",
    "print(\"NTK (ours):\\n\", mlp_ntk_ours(xs, num_hidden_layers=3, W_std=1, b_std=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "american-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stronger tests\n",
    "for num_hidden_layers in [1, 4, 20]:\n",
    "    key, _ = jax.random.split(jax.random.PRNGKey(num_hidden_layers))\n",
    "    xs = jax.random.normal(key=key, shape=(10, 5))\n",
    "    \n",
    "    assert jnp.allclose(\n",
    "        mlp_ntk_ref(xs, num_hidden_layers=num_hidden_layers,\n",
    "                     W_std=jnp.pi, b_std=jnp.e),\n",
    "        mlp_ntk_ours(xs, num_hidden_layers=num_hidden_layers,\n",
    "                      W_std=jnp.pi, b_std=jnp.e),\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
