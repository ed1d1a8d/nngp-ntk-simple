{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dress-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.config import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from neural_tangents import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "brilliant-guarantee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(861, dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get rid of Jax warnings up front\n",
    "jnp.arange(42).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-turkish",
   "metadata": {},
   "source": [
    "# Reference implementation\n",
    "Using `neural-tangents`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-disposition",
   "metadata": {},
   "source": [
    "### Diagram of residual block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "educated-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_block(\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    return stax.serial(\n",
    "        stax.FanOut(2), stax.parallel(\n",
    "            stax.Identity(),\n",
    "            stax.serial(\n",
    "                stax.Dense(1, W_std=W_std, b_std=b_std),\n",
    "                stax.Relu(),\n",
    "                stax.Dense(1, W_std=W_std, b_std=b_std),\n",
    "            )\n",
    "        ), stax.FanInSum(),\n",
    "    )\n",
    "\n",
    "def _get_kernel_fn(\n",
    "    num_res_blocks: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    layers = [\n",
    "        stax.Dense(1, W_std=W_std, b_std=b_std)\n",
    "    ] + [\n",
    "        get_res_block(W_std=W_std, b_std=b_std)\n",
    "        for _ in range(num_res_blocks)\n",
    "    ] + [stax.Relu(), stax.Dense(1, W_std=W_std, b_std=0)]\n",
    "    \n",
    "    _, _, kernel_fn = stax.serial(*layers)\n",
    "    return kernel_fn\n",
    "\n",
    "\n",
    "def rmlp_nngp_ref(\n",
    "    xs: jnp.ndarray,\n",
    "    num_res_blocks: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    kernel_fn = _get_kernel_fn(\n",
    "        num_res_blocks=num_res_blocks,\n",
    "        W_std=W_std,\n",
    "        b_std=b_std,\n",
    "    )\n",
    "    \n",
    "    return kernel_fn(xs, xs).nngp\n",
    "\n",
    "\n",
    "def rmlp_ntk_ref(\n",
    "    xs: jnp.ndarray,\n",
    "    num_res_blocks: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    kernel_fn = _get_kernel_fn(\n",
    "        num_res_blocks=num_res_blocks,\n",
    "        W_std=W_std,\n",
    "        b_std=b_std,\n",
    "    )\n",
    "    \n",
    "    return kernel_fn(xs, xs).ntk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-logging",
   "metadata": {},
   "source": [
    "We ignore the independent input warning\n",
    "and cite Tensor Programs II for correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "broadband-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/18.408/lib/python3.8/site-packages/neural_tangents/stax.py:3964: UserWarning: `FanIn` layers assume independent inputs which is not verified in the code. Please make sure to have at least one `Dense` / `Conv` / `GlobalSelfAttention` etc. layer in each branch.\n",
      "  warnings.warn('`FanIn` layers assume independent inputs which is not verified'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNGP:\n",
      " [[6.40371803 5.71130867 5.42762025]\n",
      " [5.71130867 8.54887713 5.56358456]\n",
      " [5.42762025 5.56358456 6.05370648]]\n",
      "NTK:\n",
      " [[22.67737213 13.64461843 14.63093884]\n",
      " [13.64461843 31.25800854 13.28012561]\n",
      " [14.63093884 13.28012561 21.27732594]]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(1))\n",
    "xs = jax.random.normal(key=key, shape=(3, 10))\n",
    "\n",
    "print(\"NNGP:\\n\", rmlp_nngp_ref(xs, num_res_blocks=3, W_std=1, b_std=1))\n",
    "print(\"NTK:\\n\", rmlp_ntk_ref(xs, num_res_blocks=3, W_std=1, b_std=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "macro-virus",
   "metadata": {},
   "source": [
    "# Our implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-uniform",
   "metadata": {},
   "source": [
    "### NNGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "molecular-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_relu(K: jnp.ndarray):\n",
    "    \"\"\"K should be square\"\"\"\n",
    "    sqrt = jnp.sqrt(\n",
    "        jnp.diag(K)[:, jnp.newaxis]\n",
    "      * jnp.diag(K)[jnp.newaxis, :]\n",
    "    )\n",
    "    \n",
    "    c = K / sqrt\n",
    "    \n",
    "    return 1 / (2 * jnp.pi) * (\n",
    "        jnp.sqrt(1 - c * c)\n",
    "        + (jnp.pi - jnp.arccos(c)) * c\n",
    "    ) * sqrt\n",
    "\n",
    "def rmlp_nngp_ours(\n",
    "    xs: jnp.ndarray,\n",
    "    num_res_blocks: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    assert num_res_blocks > 0\n",
    "    \n",
    "    data_dim = xs.shape[1]\n",
    "    W_var = W_std ** 2\n",
    "    b_var = b_std ** 2\n",
    "    \n",
    "    # Initial covariance in data space\n",
    "    K_init = xs @ xs.T\n",
    "    \n",
    "    # Kernel after first weight layer (no ReLU)\n",
    "    K = W_var * K_init / data_dim + b_var\n",
    "    \n",
    "    # Residual blocks\n",
    "    for i in range(num_res_blocks):\n",
    "        K += W_var * v_relu(W_var * K + b_var) + b_var\n",
    "    \n",
    "    # Final linear output layer\n",
    "    K = W_var * v_relu(K)\n",
    "    \n",
    "    return K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "tamil-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNGP (ref):\n",
      " [[ 9.45871832  6.78979695  9.30120483]\n",
      " [ 6.78979695  9.76577003  6.89616609]\n",
      " [ 9.30120483  6.89616609 11.42867631]]\n",
      "NNGP (ours):\n",
      " [[ 9.45871832  6.78979695  9.30120483]\n",
      " [ 6.78979695  9.76577003  6.89616609]\n",
      " [ 9.30120483  6.89616609 11.42867631]]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(1))\n",
    "xs = jax.random.normal(key=key, shape=(3, 5))\n",
    "\n",
    "print(\"NNGP (ref):\\n\", rmlp_nngp_ref(xs, num_res_blocks=3, W_std=1.1, b_std=0.9))\n",
    "print(\"NNGP (ours):\\n\", rmlp_nngp_ours(xs, num_res_blocks=3, W_std=1.1, b_std=0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fantastic-alexandria",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/18.408/lib/python3.8/site-packages/neural_tangents/stax.py:3964: UserWarning: `FanIn` layers assume independent inputs which is not verified in the code. Please make sure to have at least one `Dense` / `Conv` / `GlobalSelfAttention` etc. layer in each branch.\n",
      "  warnings.warn('`FanIn` layers assume independent inputs which is not verified'\n",
      "/usr/local/Caskroom/miniconda/base/envs/18.408/lib/python3.8/site-packages/neural_tangents/stax.py:3964: UserWarning: `FanIn` layers assume independent inputs which is not verified in the code. Please make sure to have at least one `Dense` / `Conv` / `GlobalSelfAttention` etc. layer in each branch.\n",
      "  warnings.warn('`FanIn` layers assume independent inputs which is not verified'\n"
     ]
    }
   ],
   "source": [
    "# Stronger tests\n",
    "for num_res_blocks in [2, 3, 20]:\n",
    "    key, _ = jax.random.split(jax.random.PRNGKey(num_res_blocks))\n",
    "    xs = jax.random.normal(key=key, shape=(10, 5))\n",
    "    \n",
    "    assert jnp.allclose(\n",
    "         rmlp_nngp_ref(xs, num_res_blocks=num_res_blocks,\n",
    "                       W_std=1.1, b_std=0.9),\n",
    "         rmlp_nngp_ours(xs, num_res_blocks=num_res_blocks,\n",
    "                        W_std=1.1, b_std=0.9),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-harvard",
   "metadata": {},
   "source": [
    "### NTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "requested-contemporary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def v_relu_prime(K: jnp.ndarray):\n",
    "    \"\"\"K should be square\"\"\"\n",
    "    sqrt = jnp.sqrt(\n",
    "        jnp.diag(K)[:, jnp.newaxis]\n",
    "      * jnp.diag(K)[jnp.newaxis, :]\n",
    "    )\n",
    "    \n",
    "    c = K / sqrt\n",
    "    \n",
    "    return 1 / (2 * jnp.pi) * (jnp.pi - jnp.arccos(c))\n",
    "\n",
    "def rmlp_ntk_ours(\n",
    "    xs: jnp.ndarray,\n",
    "    num_res_blocks: int,\n",
    "    W_std: float,\n",
    "    b_std: float,\n",
    "):\n",
    "    assert num_res_blocks > 0\n",
    "    \n",
    "    batch_size = xs.shape[0]\n",
    "    data_dim = xs.shape[1]\n",
    "    W_var = W_std ** 2\n",
    "    b_var = b_std ** 2\n",
    "    \n",
    "    # Initial covariance in data space\n",
    "    K_init = xs @ xs.T\n",
    "    \n",
    "    ############################## Begin forward pass\n",
    "    \n",
    "    embedding_K = W_var * K_init / data_dim + b_var\n",
    "    \n",
    "    # forward_Ks[i] is the coordinate covariance\n",
    "    # of the output after the ith residual block\n",
    "    # The 0th residual block is the embedding layer.\n",
    "    forward_Ks = [embedding_K]\n",
    "    for i in range(num_res_blocks):\n",
    "        prv_K = forward_Ks[-1]\n",
    "        forward_Ks.append(\n",
    "            prv_K + W_var * v_relu(W_var * prv_K + b_var) + b_var\n",
    "        )\n",
    "    \n",
    "    # out_K is the covariance of the output\n",
    "    # it is the NNGP kernel\n",
    "    out_K = W_var * v_relu(forward_Ks[-1])\n",
    "                        \n",
    "    ############################## End forward pass\n",
    "    \n",
    "    ############################## Begin backward pass\n",
    "    \n",
    "    ntk = jnp.zeros((batch_size, batch_size))\n",
    "    \n",
    "    # output layer weights\n",
    "    ntk += out_K\n",
    "    \n",
    "    # Work backwards through residual blocks\n",
    "    # cur_grad_K[i][j] gives the limit of the inner product\n",
    "    # of the gradients of the current weight layer's output\n",
    "    # for inputs xs[i] and xs[j]\n",
    "    cur_grad_K = W_var * v_relu_prime(forward_Ks[-1])\n",
    "    for i in range(num_res_blocks, 0, -1):\n",
    "        # Save a copy of original gradient\n",
    "        # to use later for the skip connection\n",
    "        skip_grad_K = cur_grad_K.copy()\n",
    "        \n",
    "        # second weight layer in block\n",
    "        ntk += cur_grad_K * v_relu(W_var * forward_Ks[i - 1] + b_var)\n",
    "        \n",
    "        # back up gradients to first weight layer of block\n",
    "        cur_grad_K *= W_var\n",
    "        cur_grad_K *= v_relu_prime(W_var * forward_Ks[i - 1] + b_var)\n",
    "        \n",
    "        # first weight layer in block\n",
    "        ntk += cur_grad_K * forward_Ks[i - 1]\n",
    "        \n",
    "        # back up gradients to beginning of block\n",
    "        cur_grad_K *= W_var\n",
    "        cur_grad_K += skip_grad_K\n",
    "        \n",
    "    # First embedding weight layer\n",
    "    ntk += cur_grad_K * embedding_K\n",
    "    \n",
    "    ############################## End backward pass\n",
    "    \n",
    "    return ntk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "broad-nelson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTK (ref):\n",
      " [[ 1.26394385 -0.05351018  0.85058322]\n",
      " [-0.05351018  1.42538696 -0.07560456]\n",
      " [ 0.85058322 -0.07560456  2.2997177 ]]\n",
      "NTK (ours):\n",
      " [[ 1.26394385 -0.05351018  0.85058322]\n",
      " [-0.05351018  1.42538696 -0.07560456]\n",
      " [ 0.85058322 -0.07560456  2.2997177 ]]\n"
     ]
    }
   ],
   "source": [
    "key, _ = jax.random.split(jax.random.PRNGKey(1))\n",
    "xs = jax.random.normal(key=key, shape=(3, 5))\n",
    "\n",
    "print(\"NTK (ref):\\n\", rmlp_ntk_ref(xs, num_res_blocks=1, W_std=1, b_std=0))\n",
    "print(\"NTK (ours):\\n\", rmlp_ntk_ours(xs, num_res_blocks=1, W_std=1, b_std=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
